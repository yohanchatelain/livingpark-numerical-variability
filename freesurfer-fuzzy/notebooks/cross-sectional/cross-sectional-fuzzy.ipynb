{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Living-park cross-sectional analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute CSV files for area, volume and thickness measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in root dir: <living-park>\n",
      "Input directory: <living-park>/vip_outputs\n",
      "Output directory: <living-park>/tables_QCed\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "from freesurfer.aparcstats2table import main as aparcstats2table\n",
    "from freesurfer.asegstats2table import main as asegstats2table\n",
    "\n",
    "anonymizer = True\n",
    "\n",
    "root_dir = Path.cwd()\n",
    "\n",
    "\n",
    "def anondir(path: Path, prefix=root_dir) -> Path:\n",
    "    \"\"\"Anonymize a directory path by replacing user-specific parts with <root>.\"\"\"\n",
    "    if not anonymizer:\n",
    "        return path\n",
    "    path_str = str(path).replace(str(prefix), \"<living-park>\")\n",
    "    return Path(path_str)\n",
    "\n",
    "\n",
    "print(f\"Running in root dir: {anondir(root_dir)}\")\n",
    "input_dir = root_dir / \"vip_outputs\"\n",
    "print(f\"Input directory: {anondir(input_dir)}\")\n",
    "output_dir = root_dir / \"tables_QCed\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {anondir(output_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build TSV tables from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subject/visit in dataset: 534\n",
      "Unique subjects in cohort: 267\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\n",
    "    root_dir\n",
    "    / \"cohort\"\n",
    "    / \"vip_executions_stats_info_2visits_passed_qc_with_26_repetitions.csv\"\n",
    ")\n",
    "cohort = pd.read_csv(root_dir / \"cohort\" / \"cross-sectional_cohort_qced.csv\")\n",
    "subjects_in_cohort = cohort[\"PATNO_id\"].unique()\n",
    "dataset = dataset[dataset[\"subject_visit\"].isin(subjects_in_cohort)]\n",
    "print(f\"Total subject/visit in dataset: {len(subjects_in_cohort)}\")\n",
    "print(f\"Unique subjects in cohort: {dataset['subject'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  34 | elapsed:    2.7s remaining:   28.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  34 | elapsed:    3.3s remaining:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  34 | elapsed:    3.4s remaining:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  34 | elapsed:    3.5s remaining:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  34 | elapsed:    3.6s remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  34 | elapsed:    5.3s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  34 | elapsed:    5.5s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  34 | elapsed:    6.0s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  34 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  34 | elapsed:    3.1s remaining:   31.6s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  34 | elapsed:    3.5s remaining:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  34 | elapsed:    3.6s remaining:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  34 | elapsed:    3.8s remaining:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  34 | elapsed:    3.9s remaining:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  34 | elapsed:    6.0s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  34 | elapsed:    6.6s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  34 | elapsed:    7.3s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  34 | elapsed:    7.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  34 | elapsed:    1.3s remaining:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  34 | elapsed:    1.5s remaining:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  34 | elapsed:    1.6s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  34 | elapsed:    1.6s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  34 | elapsed:    1.6s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  34 | elapsed:    2.3s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  34 | elapsed:    2.5s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  34 | elapsed:    2.6s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  34 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  34 | elapsed:    1.2s remaining:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  34 | elapsed:    1.4s remaining:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  34 | elapsed:    1.5s remaining:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  34 | elapsed:    1.6s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  34 | elapsed:    1.6s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  34 | elapsed:    2.2s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  34 | elapsed:    2.4s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  34 | elapsed:    2.5s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  34 | elapsed:    2.6s finished\n"
     ]
    }
   ],
   "source": [
    "measurements = [\"volume\", \"thickness\", \"area\"]\n",
    "\n",
    "rsv = {\n",
    "    f\"rep{rep}\": dataset[dataset[\"repetition\"] == rep][\"subject_visit\"].tolist()\n",
    "    for rep in dataset[\"repetition\"].unique()\n",
    "}\n",
    "\n",
    "\n",
    "def compute_aseg_volume(subjects_dir, repetition, subjects, output_dir):\n",
    "    subjects_dir = os.path.join(subjects_dir, repetition)\n",
    "    tablefile = os.path.join(output_dir, f\"{repetition}.aseg.volume.tsv\")\n",
    "    args = [\n",
    "        f\"--sd={subjects_dir}\",\n",
    "        \"--skip\",\n",
    "        \"--subjects\",\n",
    "        *subjects,\n",
    "        \"--meas=volume\",\n",
    "        f\"--tablefile={tablefile}\",\n",
    "    ]\n",
    "    asegstats2table(args)\n",
    "\n",
    "\n",
    "def compute_volume(subjects_dir, repetition, subjects, output_dir):\n",
    "    subjects_dir = os.path.join(subjects_dir, repetition)\n",
    "    for hemi in [\"lh\", \"rh\"]:\n",
    "        tablefile = os.path.join(output_dir, f\"{repetition}.{hemi}.aparc.volume.tsv\")\n",
    "        args = [\n",
    "            f\"--sd={subjects_dir}\",\n",
    "            \"--skip\",\n",
    "            \"--parc=aparc\",\n",
    "            f\"--hemi={hemi}\",\n",
    "            \"--subjects\",\n",
    "            *subjects,\n",
    "            \"--meas=volume\",\n",
    "            f\"--tablefile={tablefile}\",\n",
    "        ]\n",
    "        aparcstats2table(args)\n",
    "\n",
    "\n",
    "def compute_thickness(subjects_dir, repetition, subjects, output_dir):\n",
    "    subjects_dir = os.path.join(subjects_dir, repetition)\n",
    "    for hemi in [\"lh\", \"rh\"]:\n",
    "        tablefile = os.path.join(output_dir, f\"{repetition}.{hemi}.aparc.thickness.tsv\")\n",
    "        args = [\n",
    "            f\"--sd={subjects_dir}\",\n",
    "            \"--skip\",\n",
    "            \"--parc=aparc\",\n",
    "            f\"--hemi={hemi}\",\n",
    "            \"--subjects\",\n",
    "            *subjects,\n",
    "            \"--meas=thickness\",\n",
    "            f\"--tablefile={tablefile}\",\n",
    "        ]\n",
    "        aparcstats2table(args)\n",
    "\n",
    "\n",
    "def compute_area(subjects_dir, repetition, subjects, output_dir):\n",
    "    subjects_dir = os.path.join(subjects_dir, repetition)\n",
    "    for hemi in [\"lh\", \"rh\"]:\n",
    "        tablefile = os.path.join(output_dir, f\"{repetition}.{hemi}.aparc.area.tsv\")\n",
    "        args = [\n",
    "            f\"--sd={subjects_dir}\",\n",
    "            \"--skip\",\n",
    "            \"--parc=aparc\",\n",
    "            f\"--hemi={hemi}\",\n",
    "            \"--subjects\",\n",
    "            *subjects,\n",
    "            \"--meas=area\",\n",
    "            f\"--tablefile={tablefile}\",\n",
    "        ]\n",
    "        aparcstats2table(args)\n",
    "\n",
    "\n",
    "joblib.Parallel(n_jobs=-1, verbose=10)(\n",
    "    joblib.delayed(compute_aseg_volume)(input_dir, repetition, subjects, output_dir)\n",
    "    for repetition, subjects in rsv.items()\n",
    ")\n",
    "joblib.Parallel(n_jobs=-1, verbose=10)(\n",
    "    joblib.delayed(compute_volume)(input_dir, repetition, subjects, output_dir)\n",
    "    for repetition, subjects in rsv.items()\n",
    ")\n",
    "joblib.Parallel(n_jobs=-1, verbose=10)(\n",
    "    joblib.delayed(compute_thickness)(input_dir, repetition, subjects, output_dir)\n",
    "    for repetition, subjects in rsv.items()\n",
    ")\n",
    "_ = joblib.Parallel(n_jobs=-1, verbose=10)(\n",
    "    joblib.delayed(compute_area)(input_dir, repetition, subjects, output_dir)\n",
    "    for repetition, subjects in rsv.items()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TSV tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = output_dir / \"*.tsv\"\n",
    "tsv_tables = glob.glob(str(filename))\n",
    "aseg_tables_group = {}\n",
    "aparc_tables_group = {}\n",
    "for tsv_table in tsv_tables:\n",
    "    fields = os.path.basename(tsv_table).split(\".\")\n",
    "    if fields[1] == \"aseg\":\n",
    "        aseg_tables_group[\"volume\"] = aseg_tables_group.get(\"volume\", []) + [tsv_table]\n",
    "    elif fields[1] in [\"lh\", \"rh\"]:\n",
    "        hemi = fields[1]\n",
    "        measure = fields[3]\n",
    "        aparc_tables_group[measure] = aparc_tables_group.get(measure, {})\n",
    "        aparc_tables_group[measure][hemi] = aparc_tables_group[measure].get(\n",
    "            hemi, []\n",
    "        ) + [tsv_table]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parcellation tables found:\n",
      "\tFound 34 for volume rh tables\n",
      "\tFound 34 for volume lh tables\n",
      "\tFound 34 for thickness rh tables\n",
      "\tFound 34 for thickness lh tables\n",
      "\tFound 34 for area lh tables\n",
      "\tFound 34 for area rh tables\n",
      "Segmentation tables found:\n",
      "\tFound 34 for volume tables\n"
     ]
    }
   ],
   "source": [
    "print(\"Parcellation tables found:\")\n",
    "for measure, tables in aparc_tables_group.items():\n",
    "    for hemi, tsv_tables in tables.items():\n",
    "        print(f\"\\tFound {len(tsv_tables)} for {measure} {hemi} tables\")\n",
    "print(\"Segmentation tables found:\")\n",
    "for measure, tsv_tables in aseg_tables_group.items():\n",
    "    print(f\"\\tFound {len(tsv_tables)} for {measure} tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir: <living-park>\n"
     ]
    }
   ],
   "source": [
    "print(f\"root_dir: {anondir(root_dir)}\")\n",
    "stats_dir = Path(root_dir) / \"stats_QCed\"\n",
    "raw_stats_dir = stats_dir / \"raw\"\n",
    "os.makedirs(stats_dir, exist_ok=True)\n",
    "os.makedirs(raw_stats_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cohort_filename = root_dir / \"cohort\" / \"cross-sectional_cohort_qced.csv\"\n",
    "cohort = pd.read_csv(cohort_filename)\n",
    "\n",
    "\n",
    "def read_tsv(tsv):\n",
    "    fields = os.path.basename(tsv).split(\".\")\n",
    "    repetition = fields[0].replace(\"rep\", \"\")\n",
    "    df = pd.read_csv(tsv, sep=\"\\t\")\n",
    "    df.rename(lambda column: column.replace(\"lh_\", \"\"), inplace=True, axis=1)\n",
    "    df.rename(lambda column: column.replace(\"rh_\", \"\"), inplace=True, axis=1)\n",
    "    df.rename(columns={df.columns[0]: \"subject_visit\"}, inplace=True)\n",
    "    df = df[df[\"subject_visit\"].isin(cohort[\"PATNO_id\"].unique())]\n",
    "    df[\"subject\"] = df[\"subject_visit\"].apply(lambda x: x.split(\"_\")[0])\n",
    "    df[\"visit\"] = df[\"subject_visit\"].apply(lambda x: x.split(\"_\")[1])\n",
    "    df[\"repetition\"] = int(repetition)\n",
    "    df[\"dx_group\"] = df[\"subject_visit\"].apply(\n",
    "        lambda x: (\n",
    "            cohort[cohort[\"PATNO_id\"] == x][\"dx_group\"].values[0]\n",
    "            if x in cohort[\"PATNO_id\"].unique()\n",
    "            else \"unknown\"\n",
    "        )\n",
    "    )\n",
    "    df[\"PD_status\"] = df[\"dx_group\"].apply(lambda x: \"PD\" if \"PD\" in x else \"HC\")\n",
    "    df[\"PATNO_id\"] = df[\"subject_visit\"]\n",
    "    if \".lh.\" in tsv:\n",
    "        df[\"hemi\"] = \"lh\"\n",
    "    elif \".rh.\" in tsv:\n",
    "        df[\"hemi\"] = \"rh\"\n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_tables(tables, hemi=True):\n",
    "    if hemi:\n",
    "        lh = pd.concat([read_tsv(table) for table in tables[\"lh\"]])\n",
    "        rh = pd.concat([read_tsv(table) for table in tables[\"rh\"]])\n",
    "        concat = pd.concat((lh, rh))\n",
    "        return concat\n",
    "    else:\n",
    "        return pd.concat([read_tsv(table) for table in tables])\n",
    "\n",
    "\n",
    "thickness_df = concat_tables(aparc_tables_group[\"thickness\"])\n",
    "thickness_df.to_parquet(raw_stats_dir / \"thickness.parquet\")\n",
    "\n",
    "area_df = concat_tables(aparc_tables_group[\"area\"])\n",
    "area_df.to_parquet(raw_stats_dir / \"area.parquet\")\n",
    "\n",
    "volume_df = concat_tables(aparc_tables_group[\"volume\"])\n",
    "volume_df.to_parquet(raw_stats_dir / \"volume.parquet\")\n",
    "\n",
    "subcortical_volume = concat_tables(aseg_tables_group[\"volume\"], hemi=False)\n",
    "subcortical_volume.to_parquet(raw_stats_dir / \"subcortical_volume.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-computed tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "thickness_df = pd.read_parquet(raw_stats_dir / \"thickness.parquet\")\n",
    "area_df = pd.read_parquet(raw_stats_dir / \"area.parquet\")\n",
    "volume_df = pd.read_parquet(raw_stats_dir / \"volume.parquet\")\n",
    "subcortical_volume_df = pd.read_parquet(raw_stats_dir / \"subcortical_volume.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 26 repetitions for each subject\n",
      "\n",
      "Cortical thickness\n",
      "Keeping 26 rows per subject×hemi (hemi=True)\n",
      "Rows before filtering: 32200\n",
      "Removed rejected images: 0\n",
      "Subjects (lh): 534 | Subjects (rh): 534\n",
      "Total subjects: 534\n",
      "Final rows kept: 27768\n",
      "\n",
      "Cortical area\n",
      "Keeping 26 rows per subject×hemi (hemi=True)\n",
      "Rows before filtering: 32200\n",
      "Removed rejected images: 0\n",
      "Subjects (lh): 534 | Subjects (rh): 534\n",
      "Total subjects: 534\n",
      "Final rows kept: 27768\n",
      "\n",
      "Cortical volume\n",
      "Keeping 26 rows per subject×hemi (hemi=True)\n",
      "Rows before filtering: 32200\n",
      "Removed rejected images: 0\n",
      "Subjects (lh): 534 | Subjects (rh): 534\n",
      "Total subjects: 534\n",
      "Final rows kept: 27768\n",
      "\n",
      "Subcortical volume\n",
      "Keeping 26 rows per subject (hemi=False)\n",
      "Rows before filtering: 16100\n",
      "Removed rejected images: 0\n",
      "Total subjects: 534\n",
      "Final rows kept: 13884\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hashlib import sha1\n",
    "\n",
    "\n",
    "def keep_first_rows(\n",
    "    df: pd.DataFrame,\n",
    "    dataset: pd.DataFrame,\n",
    "    n: int = 26,\n",
    "    hemi: bool = False,\n",
    "    subject_col: str = \"subject_visit\",\n",
    "    repetition_col: str = \"repetition\",\n",
    "    hemi_col: str = \"hemi\",\n",
    "    reject_col: str = \"rejected_images\",\n",
    "    merge_left_on: tuple | list = None,\n",
    "    merge_right_on: tuple | list = None,\n",
    "    sort_by: list | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keep the first n rows per subject (and per hemisphere if hemi=True) after\n",
    "    merging with `dataset` and removing rejected images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Measurement dataframe (e.g., cortical thickness).\n",
    "    dataset : DataFrame\n",
    "        Metadata dataframe to merge (must share subject/repetition keys).\n",
    "    n : int\n",
    "        Number of rows to keep per group.\n",
    "    hemi : bool\n",
    "        If True, enforce n per (subject, hemi). Otherwise per subject.\n",
    "    subject_col, repetition_col, hemi_col, reject_col : str\n",
    "        Column names in `df`.\n",
    "    merge_left_on, merge_right_on : list/tuple\n",
    "        Keys to merge on. Defaults to [subject_col, repetition_col] for both.\n",
    "    sort_by : list | None\n",
    "        Extra sort keys (in addition to grouping keys). Defaults to [repetition_col].\n",
    "    verbose : bool\n",
    "        Print progress info.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Sampled dataframe with exactly n rows per group (subject or subject×hemi).\n",
    "        Subjects/groups with < n available rows are dropped (and reported).\n",
    "    \"\"\"\n",
    "\n",
    "    # Make safe copies and align merge columns if needed\n",
    "    left = df.copy()\n",
    "    right = dataset.copy()\n",
    "\n",
    "    # ----- Merge\n",
    "    merged = pd.merge(\n",
    "        left, right, on=[\"subject_visit\", \"subject\", \"visit\", \"repetition\"], how=\"inner\"\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Keeping {n} rows per {'subject×hemi' if hemi else 'subject'} (hemi={hemi})\"\n",
    "        )\n",
    "        print(f\"Rows before filtering: {merged.shape[0]}\")\n",
    "\n",
    "    # ----- Remove rejected images (if the column exists)\n",
    "    if reject_col in merged.columns:\n",
    "        nb_before = merged.shape[0]\n",
    "        merged = merged[~merged[reject_col].astype(bool)]\n",
    "        if verbose:\n",
    "            print(f\"Removed rejected images: {nb_before - merged.shape[0]}\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Warning: '{reject_col}' column not found; skipping rejection filter.\"\n",
    "            )\n",
    "\n",
    "    if merged.empty:\n",
    "        if verbose:\n",
    "            print(\"No data left after merge/filtering.\")\n",
    "        return merged\n",
    "\n",
    "    # ----- Sort for deterministic head(n)\n",
    "    group_keys = [subject_col] + (\n",
    "        [hemi_col] if hemi and (hemi_col in merged.columns) else []\n",
    "    )\n",
    "    if hemi and hemi_col not in merged.columns:\n",
    "        raise KeyError(f\"hemi=True but '{hemi_col}' column is missing.\")\n",
    "\n",
    "    if sort_by is None:\n",
    "        sort_by = [repetition_col] if repetition_col in merged.columns else []\n",
    "    sort_keys = [k for k in (group_keys + sort_by) if k in merged.columns]\n",
    "    if sort_keys:\n",
    "        merged = merged.sort_values(by=sort_keys)\n",
    "\n",
    "    # ----- Sample first n per group\n",
    "    sampled = merged.groupby(group_keys, as_index=False, group_keys=False).head(n)\n",
    "\n",
    "    # ----- Enforce exactly n per group (drop incomplete groups)\n",
    "    counts = sampled.groupby(group_keys).size().reset_index(name=\"k\")\n",
    "    keepers = counts[counts[\"k\"] == n][group_keys]\n",
    "    sampled_strict = sampled.merge(keepers, on=group_keys, how=\"inner\")\n",
    "\n",
    "    # Report dropped groups\n",
    "    dropped = counts[counts[\"k\"] < n]\n",
    "    if not dropped.empty and verbose:\n",
    "        dropped_subjects = dropped[subject_col].unique()\n",
    "        # Stable, short encodings for display\n",
    "        encoded = [sha1(str(s).encode()).hexdigest()[:8] for s in dropped_subjects]\n",
    "        print(f\"Groups with < {n} rows dropped: {len(dropped_subjects)} subjects\")\n",
    "        print(f\"Encoded subject ids (first 8 hex): {encoded}\")\n",
    "\n",
    "    # ----- Assertions & summary\n",
    "    if hemi:\n",
    "        # Check per-hemi counts\n",
    "        for h in sampled_strict[hemi_col].unique():\n",
    "            sub_h = sampled_strict[sampled_strict[hemi_col] == h]\n",
    "            c = sub_h.groupby(subject_col).size()\n",
    "            assert (c == n).all(), f\"Found a {hemi_col}={h} group with != {n} rows.\"\n",
    "        if verbose:\n",
    "            n_subj_lh = sampled_strict.query(f\"{hemi_col} == 'lh'\")[\n",
    "                subject_col\n",
    "            ].nunique()\n",
    "            n_subj_rh = sampled_strict.query(f\"{hemi_col} == 'rh'\")[\n",
    "                subject_col\n",
    "            ].nunique()\n",
    "            print(f\"Subjects (lh): {n_subj_lh} | Subjects (rh): {n_subj_rh}\")\n",
    "            print(f\"Total subjects: {sampled_strict[subject_col].nunique()}\")\n",
    "    else:\n",
    "        c = sampled_strict.groupby(subject_col).size()\n",
    "        assert (c == n).all(), f\"Found a subject with != {n} rows.\"\n",
    "        if verbose:\n",
    "            print(f\"Total subjects: {sampled_strict[subject_col].nunique()}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Final rows kept: {len(sampled_strict)}\")\n",
    "\n",
    "    return sampled_strict\n",
    "\n",
    "\n",
    "print(\"Keeping 26 repetitions for each subject\")\n",
    "print(\"\\nCortical thickness\")\n",
    "thickness_sampled_df = keep_first_rows(thickness_df, dataset, n=26, hemi=True)\n",
    "\n",
    "print(\"\\nCortical area\")\n",
    "area_sampled_df = keep_first_rows(area_df, dataset, n=26, hemi=True)\n",
    "\n",
    "print(\"\\nCortical volume\")\n",
    "volume_sampled_df = keep_first_rows(volume_df, dataset, n=26, hemi=True)\n",
    "\n",
    "print(\"\\nSubcortical volume\")\n",
    "subcortical_volume_sampled_df = keep_first_rows(\n",
    "    subcortical_volume_df, dataset, n=26, hemi=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled stats directory: <living-park>/stats_QCed/sampled\n"
     ]
    }
   ],
   "source": [
    "sampled_stats_dir = stats_dir / \"sampled\"\n",
    "os.makedirs(sampled_stats_dir, exist_ok=True)\n",
    "print(f\"Sampled stats directory: {anondir(sampled_stats_dir)}\")\n",
    "\n",
    "description = {\n",
    "    \"sample_size\": 26,\n",
    "    \"thickness\": \"Cortical thickness\",\n",
    "    \"area\": \"Cortical area\",\n",
    "    \"volume\": \"Cortical volume\",\n",
    "    \"subcortical_volume\": \"Subcortical volume\",\n",
    "}\n",
    "with open(sampled_stats_dir / \"description.json\", \"w\") as f:\n",
    "    json.dump(description, f, indent=4)\n",
    "\n",
    "thickness_sampled_df.to_parquet(sampled_stats_dir / \"thickness.parquet\")\n",
    "area_sampled_df.to_parquet(sampled_stats_dir / \"area.parquet\")\n",
    "volume_sampled_df.to_parquet(sampled_stats_dir / \"volume.parquet\")\n",
    "subcortical_volume_sampled_df.to_parquet(\n",
    "    sampled_stats_dir / \"subcortical_volume.parquet\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
